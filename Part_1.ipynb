{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Information Retrieval\n",
    "## Instructions\n",
    "1. Students will form teams of three people each and submit a single homework for each team in the format - ID1_ID2_ID3.ipynb\n",
    "2. Groups of four are not allowed.\n",
    "2. **Do not write your names anywhere.**\n",
    "3. For the code part: \n",
    "> **Write your code only in the mentioned sections. Do not change the code of other sections**. Do not use any imports unless we say so.\n",
    "4. For theoretical questions, if any - write your answer in the markdown cell dedicated to this task, in **English**.\n",
    "\n",
    "\n",
    "#### Deviation from the aforementioned  instructions will lead to reduced grade\n",
    "---\n",
    "\n",
    "\n",
    "## Clarifications\n",
    "1. The same score for the homework will be given to each member of the team.  \n",
    "2. The goal of this homework is to test your understanding of the concepts presented in the lectures. \\\n",
    "If a topic was not covered in detail during the lecture, you are asked to study it online on your own. \n",
    "Anyhow, we provide here detailed explanations for the code part and if you have problems - ask.\n",
    "3. Questions can be sent to the forum, you are encouraged to ask questions but do so after you have been thinking about your question. \n",
    "4. The length of the empty gaps (where you are supposed to write your code) is a recommendation (the amount of space took us to write the solution) and writing longer code will not harm your grade. We do not expect you to use the programming tricks and hacks we used to make the code shorter.   \n",
    "Having said that, we do encourage you to write good code and keep that in mind - **extreme** cases may be downgraded.  \n",
    "We also encourage to use informative variable names - it is easier for us to check and for you to understand. \n",
    "\n",
    "For your convenience, , the code has a **DEBUG** mode that you may use in order to debug with toy data.  \n",
    "It is recommended to solve the code in that mode (with efficiency in mind) and then run the code on all the data.\n",
    "**Do not forget to file the HW with DEBUG == False**.\n",
    "\n",
    "Download the \"Lyrics\" dataset from Moodle and put it in the same directory your script is.\n",
    "\n",
    "\n",
    "5. We use Python 3.7 for programming.\n",
    "6. Make sure you have all the packages and functions used in the import section. Most of it is native to Anaconda Python distribution.\n",
    "\n",
    "### Have fun !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from typing import List,Dict\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\netac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\netac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from string import punctuation, ascii_lowercase\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n",
    "**you can change this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "DEBUG = False\n",
    "\n",
    "\"\"\"\n",
    "Recommended to start with a small number to get a feeling for the preprocessing with prints (N_ROWS_FOR_DEBUG = 2)\n",
    "later increase this number for 5*10**3 in order to see that the code runs at reasonable speed, and change the CHUNK_SIZE accordinaly\n",
    "When setting Debug == False, our code implements bow.fit() in 15-20 minutes according to the tqdm progress bar. Your solution is not supposed to be much further than that.\n",
    "\"\"\"\n",
    "N_ROWS_FOR_DEBUG = 5\n",
    "CHUNCK_SIZE = 1 if DEBUG else 5*10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = Path(\"lyrics.csv\")\n",
    "BOW_PATH = Path(\"bow.csv\")\n",
    "N_ROWS = N_ROWS_FOR_DEBUG if DEBUG else None\n",
    "tqdm_n_iterations = N_ROWS//CHUNCK_SIZE +1 if DEBUG else 363*10**3//CHUNCK_SIZE + 1\n",
    "COLS = [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bag of words model\n",
    "### Implement the following methods:\n",
    "\n",
    "* `preprocess_sentence`: \n",
    "    * Lower case the word\n",
    "    * Ignores it if it's in the stopwords list\n",
    "    * Removes characters which are not in the allowed symbols\n",
    "    * Stems it and appends it to the output sentence\n",
    "    * Discards words with length <= 1\n",
    "    \n",
    "    \n",
    "* `update_counts_and_probabilities`: \n",
    "\n",
    "    * Update self.unigram count (the amount of time each word is in the text)\n",
    "    * Update self.bigram count (two consecutive word occurances)\n",
    "    * Update self.trigram count (three consecutive word occurances)\n",
    "    * Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}   \n",
    "    \n",
    "* `compute_word_document_frequency`:\n",
    "\n",
    "   * For each word count the number of docs it appears in. For example , for the word 'apple' -\n",
    "$$\\sum_{i \\in docs} I(apple \\in doc_i), I := Indicator function$$\n",
    "\n",
    "\n",
    "* `update_inverted_index_with_tf_idf_and_compute_document_norm`:\n",
    "\n",
    "    * Update the inverted index (which currently hold word counts) with tf idf weighing. We will compute tf by dividing with the number of words in each document. \n",
    "    * As we want to calculate the document norm, incrementally update the document norm. pay attention that later we apply sqrt to it to finish the process.\n",
    "\n",
    "#### The result of this code is a bag of words model that already counts for TF-IDF weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "allowed_symbols = set(l for l in ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [26:01<00:00, 21.39s/it]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence : str) -> List[str]:\n",
    "    output_sentence = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        word_to_insert = ''.join([c for c in word.lower() if c in allowed_symbols])\n",
    "        word_to_insert = stemmer.stem(word_to_insert)\n",
    "        if word_to_insert not in stop_words and len(word_to_insert)>1: output_sentence.append(word_to_insert)       \n",
    "    return output_sentence\n",
    "    \n",
    "\n",
    "def get_data_chuncks() -> List[str]:\n",
    "    for i ,chunck in enumerate(pd.read_csv(INPUT_FILE_PATH, usecols = COLS, chunksize = CHUNCK_SIZE, nrows = N_ROWS)):\n",
    "        chunck = chunck.values.tolist()\n",
    "        yield [chunck[i][0] for i in range(len(chunck))] \n",
    "\n",
    "class TfIdf:\n",
    "    def __init__(self):\n",
    "        self.unigram_count =  Counter()\n",
    "        self.bigram_count = Counter()\n",
    "        self.trigram_count = Counter()\n",
    "        self.document_term_frequency = Counter()\n",
    "        self.word_document_frequency = {}\n",
    "        self.inverted_index = {}\n",
    "        self.doc_norms = {}\n",
    "        self.n_docs = -1\n",
    "        self.sentence_preprocesser = preprocess_sentence\n",
    "        self.bow_path = BOW_PATH\n",
    "\n",
    "    def update_counts_and_probabilities(self, sentence :List[str],document_id:int) -> None:\n",
    "        sentence_len = len(sentence)\n",
    "        self.document_term_frequency[document_id] = sentence_len\n",
    "        for i,word in enumerate(sentence):\n",
    "            self.unigram_count[word] += 1\n",
    "            if i > 0: self.bigram_count[(sentence[i-1],sentence[i])] += 1\n",
    "            if i > 1: self.trigram_count[(sentence[i-2],sentence[i-1],sentence[i])] += 1\n",
    "                \n",
    "            #Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}\n",
    "            if word not in self.inverted_index: self.inverted_index[word] = {}\n",
    "            if document_id not in self.inverted_index[word]: self.inverted_index[word][document_id] = 0\n",
    "            self.inverted_index[word][document_id] += 1     \n",
    "        \n",
    "    def fit(self) -> None:\n",
    "        for chunck in tqdm(get_data_chuncks(), total = tqdm_n_iterations):\n",
    "            for sentence in chunck: #sentence is a song (string)\n",
    "                self.n_docs += 1 \n",
    "                if not isinstance(sentence, str):\n",
    "                    continue\n",
    "                sentence = self.sentence_preprocesser(sentence)\n",
    "                if sentence:\n",
    "                    self.update_counts_and_probabilities(sentence,self.n_docs)\n",
    "        self.save_bow() # bow is 'bag of words'\n",
    "        self.compute_word_document_frequency()\n",
    "        self.update_inverted_index_with_tf_idf_and_compute_document_norm()\n",
    "             \n",
    "    def compute_word_document_frequency(self):\n",
    "        for word in self.inverted_index.keys():\n",
    "            #For each word count the number of docs it appears in. For example , for the word 'apple' -\n",
    "            self.word_document_frequency[word] = len(self.inverted_index[word])\n",
    "            \n",
    "    def update_inverted_index_with_tf_idf_and_compute_document_norm(self):\n",
    "        doc_count={}\n",
    "        for word in self.inverted_index.keys():\n",
    "            for doc in self.inverted_index[word]:\n",
    "                if doc not in doc_count: doc_count[doc] = 0\n",
    "                doc_count[doc] += self.inverted_index[word][doc]\n",
    "                if doc not in self.doc_norms: self.doc_norms[doc] = 0   \n",
    "                self.doc_norms[doc] += self.inverted_index[word][doc]**2\n",
    "    \n",
    "        for word in self.inverted_index.keys():\n",
    "            for doc in self.inverted_index[word]:\n",
    "                tf = self.inverted_index[word][doc]/doc_count[doc]#self.doc_norms[doc]\n",
    "                df = self.word_document_frequency[word]\n",
    "                idf = np.log10(self.n_docs/df)\n",
    "                self.inverted_index[word][doc] = tf*idf\n",
    "\n",
    "                \n",
    "            \n",
    "    def save_bow(self):\n",
    "        pd.DataFrame([self.inverted_index]).T.to_csv(self.bow_path)\n",
    "                \n",
    "tf_idf = TfIdf()\n",
    "tf_idf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You need to run the TfIdf model without the DEBUG mode until this stage**\n",
    "\n",
    "## 1.11 Bag of words model:\n",
    "\n",
    "1. What is the computational complexity of this model, as a function of the number of docs in the corpus?\n",
    "2. How can we make this code better in terms running time (parallelization or other topics you find)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR SOLUTION HERE\n",
    "1. What is the computational complexity of this model, as a function of the number of docs in the corpus?\n",
    "\n",
    "Answer: The copmputational complexity of this model is O(|D||V|), where |D| in the number of documents and |V| is the maximum number of words in a document. Explanation:\n",
    "\n",
    "A Starting from the fit function we have the first for loop, iterating through all sentesnces=documents ==> (O|D|).\n",
    "\n",
    "   A.1 before calling a fuction we do some O(1) arithmetic operations\n",
    "   \n",
    "   A.2 Then for each iteration in the \"fit\" function \"for\" loop, we go first to the \"preprocess_sentence\" function wehre we  \n",
    "   iterate through all the words in a sentence --> maximum O(|V|) ==> O(|D||V|)\n",
    "   \n",
    "   A.3. After the above fuction, for each document we go to the \"update_counts_and_probabilities\" func. In this function we \n",
    "   first find the documents dictionary for a specific word, which takes O(1) becasue python stores dictionaries as Hash tables. \n",
    "   and after fining the list of documents for a specific word, finding if a document is within a dictionary is also done using \n",
    "   Hash table ==> O(1). We also finish the For loop.\n",
    "  Total Complexity for the For loop - O(|D||V|)\n",
    "  \n",
    "   \n",
    "B. After the For loop we use save_bow() to save the BOW ==> O(|D||V|)\n",
    "\n",
    "C. compute_word_document_frequency func - O(|V|):\n",
    "    since you go through all of the words and find the len ( O(1)) of its documnet dictinary. \n",
    " \n",
    "D. After the above, we go to the \"update_inverted_index_with_tf_idf_and_compute_document_norm\" func.\n",
    "   In this function we're going through all dictionary words keys , twice in cascade --> O(|V|) X 2 = O(|V|)\n",
    "       For each word- going through all documents in the nested dictionary --> O(|D|) ==> total O(|D||V|)\n",
    "       \n",
    "Total Complexity for the BOW model : O(|D||V|)+O(|D||V|)+O(|V|)+ O(|D||V|)= O(|D||V|)\n",
    "     \n",
    "       \n",
    "   \n",
    "2. How can we make this code better in terms running time (parallelization or other topics you find)? \n",
    "\n",
    "  \n",
    "   We Would have done it by keeping the operations done originally within for loop in the fit fuction without using different   \n",
    "   functions, avoiding the need the iterate through all of the words in a specific sentence multiple times. \n",
    "### END YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DocumentRetriever\n",
    "Not this retriever &#8595;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![dsafdsafsdafdsf](https://cdn3-www.dogtime.com/assets/uploads/2019/10/golden-cocker-retriever-mixed-dog-breed-pictures-cover-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the following methods:\n",
    "\n",
    "`reduce_query_to_counts`: given a list of words returns a counter object with words as keys and counts as values.\n",
    "\n",
    "`rank`: given a query and relevant documents calculate the similarity (cosine or inner product simmialrity) between each document and the query.   \n",
    "Make sure to transform the query word counts to tf idf as well. \n",
    "\n",
    "`sort_and_retrieve_k_best`: returns the top k documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    def __init__(self, tf_idf):\n",
    "        self.sentence_preprocesser = preprocess_sentence  \n",
    "        self.vocab = set(tf_idf.unigram_count.keys())\n",
    "        self.n_docs = tf_idf.n_docs\n",
    "        self.inverted_index = tf_idf.inverted_index\n",
    "        self.word_document_frequency = tf_idf.word_document_frequency\n",
    "        self.doc_norms = tf_idf.doc_norms\n",
    "    def rank(self,query : Dict[str,int],documents: Dict[str,Counter],metric: str ) -> Dict[int, float]:\n",
    "        result = {} # key: DocID , value : float , similarity to query\n",
    "        query_len = np.sum(np.array(list(query.values())))\n",
    "#given a query and relevant documents calculate the similarity (cosine or inner product simmialrity) \n",
    "#between each document and the query.\n",
    "#Make sure to transform the query word counts to tf idf as well.\n",
    "        ### YOUR CODE HERE\n",
    "        #TF IDF ON query:\n",
    "        for word in query.keys():\n",
    "            query[word]=((query[word]/query_len)*np.log10(self.n_docs/self.word_document_frequency[word])) \n",
    "        d_norm={}\n",
    "        q_norm = np.sum(np.array(list(query.values()))**2)\n",
    "        for word in documents.keys():\n",
    "            #for word in self.inverted_index.keys():\n",
    "            for doc in documents[word]:\n",
    "                if doc not in result: result[doc] = 0 \n",
    "                if word in query: result[doc] += documents[word][doc]*query[word]\n",
    "                if doc not in d_norm: d_norm[doc] = 0\n",
    "                d_norm[doc] += documents[word][doc]**2\n",
    "         ### END YOUR CODE\n",
    "        if metric == 'cosine':\n",
    "        ### YOUR CODE HERE\n",
    "            for doc in result.keys():\n",
    "                result[doc] = result[doc]/(np.sqrt(q_norm*d_norm[doc]))\n",
    "        ### END YOUR CODE\n",
    "        return result\n",
    "           \n",
    "    def sort_and_retrieve_k_best(self, scores: Dict[str, float],k :int):\n",
    "        #returns the top k documents.\n",
    "        return list(dict(sorted(scores.items(), key=lambda item: item[1],reverse=True)[:k]).keys())\n",
    "    \n",
    "    def reduce_query_to_counts(self, query : List)->  Counter:\n",
    "        #given a list of words returns a counter object with words as keys and counts as values.\n",
    "        return Counter(query)\n",
    "        \n",
    "    def get_top_k_documents(self,query : str, metric: str , k = 5) -> List[str]:\n",
    "        query = self.sentence_preprocesser(query)\n",
    "        query = [word for word in query if word in self.vocab] # filter nan \n",
    "        query_bow = self.reduce_query_to_counts(query)\n",
    "        relavant_documents = {word : self.inverted_index.get(word) for word in query}\n",
    "        ducuments_with_similarity = self.rank(query_bow,relavant_documents, metric)\n",
    "        return self.sort_and_retrieve_k_best(ducuments_with_similarity,k)\n",
    "        \n",
    "dr = DocumentRetriever(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "query = \"Better stop dreaming of the quiet life, 'cause it's the one we'll never know And quit running for that runaway bus 'cause those rosy days are few And stop apologizing for the things you've never done 'Cause time is short and life is cruel but it's up to us to change This town called malice\"\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247101, 247085, 155816, 174957, 313127]\n",
      "[66192, 61393, 128160, 309297, 318295]\n"
     ]
    }
   ],
   "source": [
    "cosine_top_k = dr.get_top_k_documents(query, 'cosine')\n",
    "print(cosine_top_k)\n",
    "inner_product_top_k = dr.get_top_k_documents(query, 'inner_product')\n",
    "print(inner_product_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "song #0 \n",
      "Yeah, I want to dedicate this song to Jonathan Myers,\n",
      "Morris Peterson, Gemini Smith, and Matthew Hingle.\n",
      "What's on yo mind?\n",
      "[Shoestring]\n",
      "No demonstration on this nation, as a murderfest\n",
      "Got us locked in the jailcell, the others they was put to rest\n",
      "I had no teacher, it was like my pops had passed away\n",
      "Bought me a sweet and snapped his fingers, he was gone away\n",
      "My house is hell, I used bail down there on ?Acre? street\n",
      "Whole hood on ABC, pops in penetentiary\n",
      "Caught in the system, he's a victim of his shorty's past\n",
      "His son's a killa on the for reala, you best to watch yo stash\n",
      "What's on my mind, is my brotha's name Rodney King?\n",
      "Coulda been Shoestring, instead the devil chose Malice Green\n",
      "Can't go to sleep, not too deep cause I be hearin shots,\n",
      "Down on my block bodies drop, it'll never stop\n",
      "The ghetto drama for yo mama is a wicked sin\n",
      "God save her soul, don't wanna say it but my mom's a fiend\n",
      "Stand in the rain, can't take the pain, the stress is kickin in\n",
      "Mack's in the pen cause it was all about his dividends\n",
      "Life was a struggle, had to hustle, and sometimes buckle\n",
      "A swollen knuckle, lockin up was the ghetto couple\n",
      "Out to get rich, but I'm no snitch, no need to drop a dime,\n",
      "My future's blind, now tell me what's on yo mind\n",
      "Chorus: Tell me, tell me, what's on yo mind? (2X)\n",
      "(What's on my mind, what's on my mind,\n",
      "Was it the chrome, too ??? the crime?)\n",
      "[Bootleg]\n",
      "Apply the pressure, drastic measures made the victim's fall\n",
      "One shot to the head, before they fled, they made em beg and crawl\n",
      "Can't stop the thunder in my mind, so who controls the storm?\n",
      "I fill my body full of drank and dank to keep it warm\n",
      "Please stop the killin, Lord a killa's what I'm born to be\n",
      "My mind's on murder God, homicides are all a see\n",
      "Please set me free from all the enemies that haunt my mind\n",
      "Why do the righteous, poor, and black suffer all the time?\n",
      "My mother talks to me, and tells me, \";Stop the violent killin\";\n",
      "Workin hard all day, tryin to make my pay\n",
      "Now how you think I'm feelin?\n",
      "What's on my mind, it's sad, look so small in kid's faces\n",
      "Knowin their daddy's doin 20 for some drug cases\n",
      "Never knew my daddy so I never could respect a man\n",
      "Learned to cook up drugs and hold my ground while other yougsters ran\n",
      "Gotta be a man, so my plan is to pursue my dreams\n",
      "My family's gotta eat so I'm gon keep on feedin fiends\n",
      "Know what I mean, the same routine almost everyday\n",
      "Law's pushin me, so I'm gon keep on stackin hay\n",
      "Out to get rich, but I'm no snitch, no need to drop a dime,\n",
      "My future's blind, now tell me, what's on yo mind?\n",
      "Chorus: Tell me, tell me, what's on yo mind? (2X)\n",
      "(What's on my mind, what's on my mind,\n",
      "Was it the chrome, too ??? the crime?)\n",
      "[Night and Day]\n",
      "Try to stop these fires, but they got me trapped inside the fence\n",
      "Wanna represent his death, cause murder's what I'm up against\n",
      "It makes no sense to me, the troubles that run through my head\n",
      "Wakin up in the mornin, knowin the grave might become my bed\n",
      "I shoulda fled, but from my problems, I can't get away\n",
      "No matter when or where I go, they're with me everyday\n",
      "I'm shootin the dice and drinkin the liquor to set my mind free\n",
      "And tryin to find a piece of my mind, where problems can't find me\n",
      "No matter what, I'm stuck, my mind is trapped inside the sin\n",
      "So I release my anger through a chamber, gin, a pad, and pin\n",
      "Your so called friends\n",
      "ain't really your friends because they don't stay true\n",
      "Besides the smokin and drinkin, now tell me, are they there for you?\n",
      "Cash or credit, gotta get it, can't be po no mo\n",
      "Some gangstas roll into the heaven's say no roll no mo\n",
      "Out to get rich, but I'm no snitch, no need to drop a dime,\n",
      "My future's blind, now tell, me what's on yo mind? \n",
      "##################################################\n",
      "##################################################\n",
      "song #1 \n",
      "Yeah, I want to dedicate this song to jonathan myers,\n",
      "Morris peterson, gemini smith, and matthew hingle.\n",
      "What's on yo mind?\n",
      "[shoestring]\n",
      "No demonstration on this nation, as a murderfest\n",
      "Got us locked in the jailcell, the others they was put to rest\n",
      "I had no teacher, it was like my pops had passed away\n",
      "Bought me a sweet and snapped his fingers, he was gone away\n",
      "My house is hell, I used bail down there on ? acre? street\n",
      "Whole hood on abc, pops in penetentiary\n",
      "Caught in the system, he's a victim of his shorty's past\n",
      "His son's a killa on the for reala, you best to watch yo stash\n",
      "What's on my mind, is my brotha's name rodney king?\n",
      "Coulda been shoestring, instead the devil chose malice green\n",
      "Can't go to sleep, not too deep cause I be hearin shots,\n",
      "Down on my block bodies drop, it'll never stop\n",
      "The ghetto drama for yo mama is a wicked sin\n",
      "God save her soul, don't wanna say it but my mom's a fiend\n",
      "Stand in the rain, can't take the pain, the stress is kickin in\n",
      "Mack's in the pen cause it was all about his dividends\n",
      "Life was a struggle, had to hustle, and sometimes buckle\n",
      "A swollen knuckle, lockin up was the ghetto couple\n",
      "Out to get rich, but I'm no snitch, no need to drop a dime,\n",
      "My future's blind, now tell me what's on yo mind\n",
      "Chorus: tell me, tell me, what's on yo mind? (2x)\n",
      "(what's on my mind, what's on my mind,\n",
      "Was it the chrome, too ? ? ? the crime? )\n",
      "[bootleg]\n",
      "Apply the pressure, drastic measures made the victim's fall\n",
      "One shot to the head, before they fled, they made em beg and crawl\n",
      "Can't stop the thunder in my mind, so who controls the storm?\n",
      "I fill my body full of drank and dank to keep it warm\n",
      "Please stop the killin, lord a killa's what I'm born to be\n",
      "My mind's on murder god, homicides are all a see\n",
      "Please set me free from all the enemies that haunt my mind\n",
      "Why do the righteous, poor, and black suffer all the time?\n",
      "My mother talks to me, and tells me, \"stop the violent killin\"\n",
      "Workin hard all day, tryin to make my pay\n",
      "Now how you think I'm feelin?\n",
      "What's on my mind, it's sad, look so small in kid's faces\n",
      "Knowin their daddy's doin 20 for some drug cases\n",
      "Never knew my daddy so I never could respect a man\n",
      "Learned to cook up drugs and hold my ground while other yougsters ran\n",
      "Gotta be a man, so my plan is to pursue my dreams\n",
      "My family's gotta eat so I'm gon keep on feedin fiends\n",
      "Know what I mean, the same routine almost everyday\n",
      "Law's pushin me, so I'm gon keep on stackin hay\n",
      "Out to get rich, but I'm no snitch, no need to drop a dime,\n",
      "My future's blind, now tell me, what's on yo mind?\n",
      "Chorus (2x)\n",
      "[night and day]\n",
      "Try to stop these fires, but they got me trapped inside the fence\n",
      "Wanna represent his death, cause murder's what I'm up against\n",
      "It makes no sense to me, the troubles that run through my head\n",
      "Wakin up in the mornin, knowin the grave might become my bed\n",
      "I shoulda fled, but from my problems, I can't get away\n",
      "No matter when or where I go, they're with me everyday\n",
      "I'm shootin the dice and drinkin the liquor to set my mind free\n",
      "And tryin to find a piece of my mind, where problems can't find me\n",
      "No matter what, I'm stuck, my mind is trapped inside the sin\n",
      "So I release my anger through a chamber, gin, a pad, and pin\n",
      "Your so called friends\n",
      "Ain't really your friends because they don't stay true\n",
      "Besides the smokin and drinkin, now tell me, are they there for you?\n",
      "Cash or credit, gotta get it, can't be po no mo\n",
      "Some gangstas roll into the heaven's say no roll no mo\n",
      "Out to get rich, but I'm no snitch, no need to drop a dime,\n",
      "My future's blind, now tell, me what's on yo mind? \n",
      "##################################################\n",
      "##################################################\n",
      "song #2 \n",
      "Creeping down the hallway quiet as kept\n",
      "the only sign of a murder was the blood on the foresteps\n",
      "I stopped for a second to wipe it up\n",
      "and threw the bloody towel in the garbage bag with her guts\n",
      "pretty as a picture her name was Rosie\n",
      "had to kill the bitch 'cause she was getting too fuckin nosey\n",
      "a school hoe she attended you of H\n",
      "a law student who was looking for a fuckin' case\n",
      "but she was barking up the wrong tree g\n",
      "ay yo why in the hell did the bitch want to fuck with me\n",
      "walking around my crib steady casin'\n",
      "askin' about the strange smells that were coming from my basement\n",
      "she asked one too many motherfuckin questions it was time\n",
      "somebody taught the stupid bitch a good lesson\n",
      "I snuck in the house by the back door\n",
      "it was like a scene from psycho\n",
      "the bitch was in the shower\n",
      "I rushed her quick so she wouldn't have a chance to holler\n",
      "and said \"shut the fuck up hoe\"\n",
      "and slammed her motherfucking face against the cold floor\n",
      "struggling soaking wet\n",
      "I gagged her mouth with a whole box of kotex\n",
      "after i fucked her check out what i did\n",
      "slit her fucking stomach and watched her squeal like a pig\n",
      "the shit was gruesome g i couldn't call it\n",
      "i cut off her fingers and flushed them down the fuckin' toilet\n",
      "and wrote my name on the wall like i usually do\n",
      "to mark a murder hoe, yeah on murder avenue\n",
      "more murder, more murder, more murder, yo\n",
      "more murder, more murder, more murder, watch me hurt a hoe\n",
      "more murder, more murder, more murder, nigga\n",
      "more motherfucking murder gots ta pull the trigger\n",
      "more murder, more murder, more murder, check it\n",
      "a hundred and fifty seven thousand victims in a second\n",
      "gotta give it up for brigitte and ted brand new newly-weds\n",
      "there's nothing i would love better than to have their fuckin' heads\n",
      "on a platter i watch them sonofabitches scatter\n",
      "in broad daylight but yo it really didn't matter\n",
      "i put my gin to their heads and said \"shut up\"\n",
      "the nigga was big i watched this big motherfucker nut up\n",
      "on the rampage both of 'em got pistol-whipped\n",
      "the 9 was bloody so i pulled out my pistol grip\n",
      "the nigga was damn near dead\n",
      "i grabbed the bitch by her head and told her \"spread your fucking legs\"\n",
      "I placed the barrel of my 9 on her pearl tongue\n",
      "and stuck a shell inside her pussy and said \"now ain't that fun?\"\n",
      "she started to cry\n",
      "I saw a tear fall from here eye i said \"bitch you must want to die\"\n",
      "I pulled the trigger of the gun back slowly\n",
      "and shot up her nigga until he was full of holes g\n",
      "the bitch was screaming with rage\n",
      "I stamped on her motherfucking face until it caved in\n",
      "'cause killing is so damn sweet\n",
      "I saved the remains and used them later for ground meat\n",
      "being a lunatic i gotta do the lunatic\n",
      "gotta do man, yeah living on this avenue\n",
      "more murder, more murder, more murder, yo\n",
      "more murder, more murder, more murder, watch me hurt a hoe\n",
      "more murder, more murder, more murder, nigga\n",
      "more motherfucking murder gots ta pull the trigger\n",
      "more murder, more murder, more murder, check it\n",
      "a hundred and fifty seven thousand victims in a second \n",
      "##################################################\n",
      "##################################################\n",
      "song #3 \n",
      "Four rooms, a ceiling and a floor but there's more\n",
      "(close to insanity)\n",
      "A desk with a subtle light, a window and a door\n",
      "(close to insanity)\n",
      "One bottle of the bluest inks your iris ever saw\n",
      "(close to insanity)\n",
      "One child prodigy with a vision in his core\n",
      "(close to insanity)\n",
      "(yeah, yo)\n",
      "I'm frost bitten, slippin' away titanic burden nurses\n",
      "Where the anti-hero clergy purge their value burning service\n",
      "And warped was I huddled beneath the influenza fresh\n",
      "Meshed with impressions that appear to shrink before my very breath\n",
      "These tides of woe and malice and mirth initiate a wave crash\n",
      "Splashing my offspring graves prior to birth it's looking bleak\n",
      "Malarky farce sergeant crooked and sleek emerald eyes glow\n",
      "I'm shook in a freak side show\n",
      "OK I strobe effects projective when I blink\n",
      "So I resign this chorus line\n",
      "When linked we let our eyelids fall and pilots stall\n",
      "With what I sing I'll open lash light and dark clash to dim the wattage\n",
      "Then see the wide eyed dry grays and supplied fiery colossus\n",
      "Well I am a hostage guiding yet pushed beneath the crazes climate\n",
      "Hiding behind the levy while the stubborn rivers rise and feel this\n",
      "I wish heavenly brevity centered hate pedigrees instead of dead serenity\n",
      "God damn must have remembered me\n",
      "It clinched me, it wrenched me, tempted me to employ it\n",
      "Apprehended me and rendered me suspended in its voyage\n",
      "How these tables have turned\n",
      "Hand to the bottle with the skull and cross bones scribbled off the label\n",
      "Sip the ladle drank the burn begging for dead\n",
      "Concerns off with a zephyr tread and leg in a web\n",
      "Caught triple-six couriers beckon they fled\n",
      "OK OK I get it\n",
      "Let 'em shake a little then release 'em\n",
      "Like as if ghostly hysterics would leash on banded completion\n",
      "Odium, patience ran his anti-death commando\n",
      "Just a litigant stretchin' to touch tranquil but couldn't quite catch the angle\n",
      "I'm trained as corner stone famine troopers\n",
      "So my tray within a heart of hearts still belly up and parched, come on\n",
      "(yeah, yeah, check it)\n",
      "I'm a sideline observer alerted not yet retreating\n",
      "(close to insanity)\n",
      "The climate stubbornly hovered slightly above freezing\n",
      "(close to insanity)\n",
      "Now everybody in the populis awaited my reply\n",
      "(close to insanity)\n",
      "I spit a billion tiny brilliant white lights into the sky\n",
      "(close to insanity)\n",
      "Undeniably amused by the way the fuse burn\n",
      "By the way the clues churn in front of my eyes\n",
      "To fertilize germination of concern for me for we\n",
      "For he who's sucked into the trench fully dug\n",
      "I don't wanna pull the plug\n",
      "Hug on my canteen like in a dream\n",
      "Centipede leader speedin' through a meaty greed league\n",
      "I can tell by the way the needs bleed from a seed\n",
      "If the breed should have ever been bread 'nuff said\n",
      "Whether compared to caterpillar and cocoon\n",
      "To emerge or a spark's soon a bloom to a surge\n",
      "All I need is the nourishment, the courage and the burn\n",
      "To ascend from a number to grave blade runner\n",
      "Hunter, cleric, swordsman, king\n",
      "More like I'm walking with a broken mood ring\n",
      "Mood swingin' from the mezzanine level\n",
      "Here to bevel the edge\n",
      "My team's settled on the ledge to pledge\n",
      "It's like that.\n",
      "In the summer it rains buckets of hunger pains\n",
      "In the winter it's the same with an added climate change\n",
      "The remaining two quadrants balance the polar values equally for midrange\n",
      "Yet the songs of thirst remain the same\n",
      "You could turn the whole cold reservoir to liquor\n",
      "Hell, split the ocean on its seams if it boosts your esteem\n",
      "I never lend span of attention lest my brethren signal fresh\n",
      "So do your magic miracle worker I'll remain unimpressed\n",
      "For the flux, the fix, the famine\n",
      "For the fact that little Billy up the block obtained a lovely hand cannon\n",
      "I'd examined auto pilot (right) when filibuster won (yeah)\n",
      "Concluded the few we're tuned with were now targets of his movement (oh shit)\n",
      "It's intriguing, yet I guess I knew somewhere something was leaking\n",
      "Now I honor instinct delinquent\n",
      "Bring settler runaways frayed boogie bastard clicks\n",
      "To bypass glass stature walking graph characters\n",
      "Militant dance split the sun and sip the filament\n",
      "Tracer, vivisection is to lab rat primes\n",
      "They try to grace these sacred lips with his maze or a dirty wine\n",
      "He knew, he brewed the substance just to mock the lesser budgets\n",
      "Then sought off all trickery bought off the public and screamed victory\n",
      "Tunnel through the mite infested grillage and the rig\n",
      "As fast a Aesop and his ten little fatigued fingers could dig trigger revenge\n",
      "Tip the goblet in the dirt review my words spit in the puddle\n",
      "Peace to faint struggle the fuck out and duck out\n",
      "(yeah, check, huh, uh)\n",
      "Now, all hail defenders of the trash talk\n",
      "(close to insanity)\n",
      "I was hidden, yet I slid in just to rip the mask off\n",
      "(close to insanity)\n",
      "I'm seventy-six inches of all the purest sounds\n",
      "(close to insanity)\n",
      "So y'all could dig me six feet deep my eyes would still be over ground\n",
      "(close to insanity)\n",
      "(It's like that) \n",
      "##################################################\n",
      "##################################################\n",
      "song #4 \n",
      "Chorus:\n",
      "[Skam]\n",
      "Yo, I'm tryin' to be the man of the day\n",
      "Three six five a year\n",
      "See the bullshit you sayin', Skam ain't tryin' to hear (Fuck that!)\n",
      "Bustin' lyrics in the air\n",
      "Keepin' some up in the chamber\n",
      "[Eminem]\n",
      "Yo Skam, what the fuck you doin'?\n",
      "[Skam]\n",
      "Man, I'm releasin anger!\n",
      "[Eminem]\n",
      "I'm tryin' to be the illest of the day\n",
      "Three six five a year\n",
      "See that bullshit you sayin', Shady ain't tryin' to hear\n",
      "Spittin' lyrics in your ear\n",
      "Keepin' some up in the chamber\n",
      "[Skam]\n",
      "Aiyyo Shady what the fuck you doin'?\n",
      "[Shady]\n",
      "I'm releasin anger!\n",
      "[Skam]\n",
      "Man I'm tired of bein' tired, everytime I wake up\n",
      "Tired of these fake ducks, tired of bein' late for the bus\n",
      "Tired of all be blendin', and endin' up datin sluts\n",
      "So my facial expression stay stuck up, and shut the fuck up!\n",
      "To the pity ass rappers, that ain't shit with out make-up, wake up\n",
      "Gold diggin' bitches, the buck stops here\n",
      "The road to riches is closed for repair\n",
      "So if the shoe ain't fittin, girl leave that shit alone\n",
      "You aint Erykah Badu, what I look like Tyrone?\n",
      "I rip your tissue out your ear\n",
      "Spittin' like I don't care\n",
      "My hair look like I ain't come it, shit a damn near year\n",
      "So I burn, zig-zag, I leave the next man with his heels up\n",
      "Brain dead, and be reincarnated at a speed bump\n",
      "When we done we stay hard, so you don't land shit\n",
      "Suck Skam's dick, off of what a dead man spit\n",
      "Yo that's it, I don't plan shit, you know how we go down\n",
      "They need to slow down, and take a look who the hoe now\n",
      "At every industry party, gettin' so damn drunk\n",
      "Can't remember the lass ass you kissed, or dick you sucked\n",
      "Remind me of my ex, in the street got me veck\n",
      "Tryin' to roll up on Miami, leavin' with a broke neck\n",
      "Oh shit, leathal lyric equal land mind\n",
      "I be stackin' up white rappers like im throwin' gas signs\n",
      "And I'm, buck wild exposin the plain truth\n",
      "You couldn't mess with me, fuckin' shit up in the same room\n",
      "Hey you, I don't know you but fuck it, let me get a dollar\n",
      "For this bad car, that go along with the breath\n",
      "Some show the mad bomb, and steal the show like a theft\n",
      "Cause in Miami, the baby jammin like three up in the chamber\n",
      "(Yo Skam what the fuck you doin?)\n",
      "Chorus\n",
      "[Eminem]\n",
      "Some people say I'm strange, I tell them ain't shit change\n",
      "I'm still the same lame asshole with a different name\n",
      "Became late to the last show with a different dane\n",
      "Brain ate from the last \"O\" that I snifed off caine\n",
      "You know you're spaced the fuck out like George Lucas\n",
      "When your puke is turnin' to yellowish-orange mucus\n",
      "So when I grab a pencil and squeeze it between fingers\n",
      "I'm not a rapper, I'm a demon who speaks English\n",
      "Freak genius, too extreme for the weak and squeamish\n",
      "Burn you alive till you screamin' to be extinguished\n",
      "Cause when I drop the science, motherfuckers tell me to stop the violence\n",
      "Start a fire and block the hydrants\n",
      "I'm just a mean person, you never seen worse than\n",
      "So when Slim gets this M-16 burstin'\n",
      "You gettin' spun backwards like every word of obscene cursin\n",
      "On the B-side of my first single with the clean version\n",
      "Stoppin your short life while you're still a teen virgin\n",
      "Unless you get a kidney specialist and a spleen surgeon\n",
      "In the best hospital possible for emergancy surgery\n",
      "To try to stop the blood from your ruptured sternum iternally\n",
      "I'll take it back before we knew each other's name\n",
      "Run in a ultrasound and snatch you out your mothers frame\n",
      "I'll take it further back than that\n",
      "Back to Lovers' lane, to the night you were thought of, and\n",
      "Cock-block your father's game\n",
      "I'll plead the fifth like my jaws were muzzled\n",
      "So suck my dick while I take a shit and do this crossword puzzle\n",
      "And when I'm down with ten seconds left in the whole bout\n",
      "I'ma throw a head-butt so hard, I'll knock us both out\n",
      "Chorus \n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "for index, song in enumerate(pd.read_csv(INPUT_FILE_PATH,usecols = [5]).iloc[cosine_top_k]['lyrics']):\n",
    "    sep = \"#\"*50\n",
    "    print(F\"{sep}\\nsong #{index} \\n{song} \\n{sep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 term statistics:\n",
    "Use \"tf_idf\" object that we created earlier and answer the following questions:\n",
    "\n",
    "1. How many unique words we have?\n",
    "2. How many potential word bigrams we have (depend on the unique words we have)? How many actual word bigrams we have? How do you explain this difference?\n",
    "3. What is the storage size of the input file \"lyrics.csv\"? What is the output file (bow.csv) size? how do you explain this difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Unique words: 387,630\n",
      "\n",
      "There are 387,630 unique words.\n",
      "\n",
      "2.\n",
      "Potential word bigrams: 300,514,033,800\n",
      "Actual word bigrams: 6,288,505\n",
      "\n",
      "We dont have all the combinations in the corpus.\n",
      "The reason is because not all of the combinations are make sense and even out of those that make sense, \n",
      "not of all of them are needed in the content of this corpus.\n",
      "\n",
      "3.\n",
      "Size of input file is: 324,632,382 bytes\n",
      "Size of output file is: 197,574,542 bytes\n",
      "\n",
      "The explanation of the diffrence is that the data in these two files is stored differently.\n",
      "In the input file we are holding every occurrence of a word explicitly.\n",
      "In the output file we are holding each word just once with list of the relevant documents for this word, with the ranks. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. \n",
    "### YOUR SOLUTION HERE\n",
    "print('1.\\nUnique words: {}'.format(f'{len(dr.vocab):,}'))\n",
    "### END YOUR SOLUTION \n",
    "print('\\nThere are {} unique words.'.format(f'{len(dr.vocab):,}'))\n",
    "\n",
    "\n",
    "# 2.\n",
    "### YOUR SOLUTION HERE\n",
    "print('\\n2.\\nPotential word bigrams: {}'.format(f'{len(dr.vocab)**2*2:,}'))\n",
    "print('Actual word bigrams: {}'.format(f'{len(tf_idf.bigram_count.keys()):,}'))\n",
    "### END YOUR SOLUTION\n",
    "print(\"\"\"\\nWe dont have all the combinations in the corpus.\n",
    "The reason is because not all of the combinations are make sense and even out of those that make sense, \n",
    "not of all of them are needed in the content of this corpus.\"\"\")\n",
    "\n",
    "\n",
    "# 3.\n",
    "### YOUR SOLUTION HERE\n",
    "print('\\n3.\\nSize of input file is: {} bytes'.format(f'{INPUT_FILE_PATH.stat().st_size:,}'))\n",
    "print('Size of output file is: {} bytes'.format(f'{BOW_PATH.stat().st_size:,}'))\n",
    "### END YOUR SOLUTION\n",
    "print(\"\"\"\\nThe explanation of the diffrence is that the data in these two files is stored differently.\n",
    "In the input file we are holding every occurrence of a word explicitly.\n",
    "In the output file we are holding each word just once with list of the relevant documents for this word, with the ranks. \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 NgramSpellingCorrector\n",
    "Now we will implement a Ngarm (character Ngrams) spelling corrector. That is, we have an out of vocabulary word (v) and we want to retrieve the most similar words (in our vocabulary) to this word.\n",
    "we will model the similarity of two words by-\n",
    "\n",
    "$$sim(v,w) := prior \\cdot likelihood = p(w) \\cdot P(v|w) $$ \n",
    "$$P(v|w) := JaccardIndex =  \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "Where v is an out of vocabulary word (typo or spelling mistake), w is in a vocabulary word, X is the ngram set of v and Y is the ngram set of w.\n",
    "For example, if n == 3, the set of ngrams for word \"banana\" is set(\"ban\",\"ana\",\"nan\",\"ana\") = {\"ban\",\"ana\",\"nan\"}\n",
    "\n",
    "In order to do it efficently, we will first construct an index from the possible Ngrams we have seen in our corpus to the words that those Ngrams appear in, in order prevent comparing v to all of the words in our corpus.\n",
    "Then, we will implement a function that computes this similarity.\n",
    "\n",
    "* Make sure you compute the JaccardIndex efficently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfor example - get_bigrams is a generator, which is an object we can loop on:\\nfor ngram in get_bigrams(word):\\n    DO SOMETHING\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 2):\n",
    "        yield \"\".join(list(ngram))\n",
    "    \n",
    "def get_trigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 3):\n",
    "        yield \"\".join(list(ngram))\n",
    "        \n",
    "\"\"\" \n",
    "for example - get_bigrams is a generator, which is an object we can loop on:\n",
    "for ngram in get_bigrams(word):\n",
    "    DO SOMETHING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramSpellingCorrector:\n",
    "    def __init__(self, unigram_counts: Counter, get_n_gram: callable):\n",
    "        self.unigram_counts = unigram_counts\n",
    "        self.ngram_index = {}\n",
    "        self.get_n_grams = get_n_gram\n",
    "    \n",
    "    def build_index(self) -> None:\n",
    "#In order to do it efficently, we will first construct an index from the possible Ngrams we have seen in our corpus \n",
    "#to the words that those Ngrams appear in, in order prevent comparing v to all of the words in our corpus. \n",
    "#Then, we will implement a function that computes this similarity.  \n",
    "        for word in self.unigram_counts:\n",
    "            for gram in self.get_n_grams(word):\n",
    "                if gram not in self.ngram_index: self.ngram_index[gram] = []\n",
    "                self.ngram_index[gram].append(word)\n",
    "             \n",
    "    def get_top_k_words(self,word:str,k=5) -> List[str]:\n",
    "        ### YOUR CODE HERE\n",
    "        result = {} \n",
    "        x = set(self.get_n_grams(word))\n",
    "        relavant_words = [v for k, v in self.ngram_index.items() if k in x] #this is list of lists\n",
    "        relavant_words = [j for i in relavant_words for j in i] #merge list of lists into one big list\n",
    "        for w in set(relavant_words):\n",
    "            y = set(self.get_n_grams(w))\n",
    "            jaccard_index = len(x.intersection(y))/len(x.union(y))\n",
    "            #result[w] = jaccard_index\n",
    "            result[w] = jaccard_index*self.unigram_counts[w]##Or's addition of *p(w)\n",
    "        return list(sorted(result.items(), key=lambda item: item[1],reverse=True)[:k])\n",
    "\n",
    "class BigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_bigrams)\n",
    "        \n",
    "        \n",
    "class TrigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_trigrams)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 9950.375),\n",
       " ('caus', 9251.354838709678),\n",
       " ('life', 7831.677419354838),\n",
       " ('still', 7501.451612903225),\n",
       " ('time', 6545.625)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_vocab_word = 'supercalifragilisticexpialidocious'\n",
    "bigram_spelling_corrector = BigramSpellingCorrector(tf_idf.unigram_count)\n",
    "bigram_spelling_corrector.build_index()\n",
    "bigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 3793.46875),\n",
       " ('still', 2348.939393939394),\n",
       " ('call', 2177.375),\n",
       " ('listen', 1374.5454545454545),\n",
       " ('hous', 565.34375)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_spelling_corrector = TrigramSpellingCorrector(tf_idf.unigram_count)\n",
    "trigram_spelling_corrector.build_index()\n",
    "trigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Language model\n",
    "Calculate the log likelihood of a sentence. Once with a bigram markovian langauge model, and once with a trigram model.\n",
    "for example - the likelihood of the senetence \"spiderman spiderman does whatever a spider can\" for the bigram model is: \n",
    "$$p(spiderman)\\cdot p(spiderman|spiderman) \\cdot  p(does|spiderman) \\cdot p(whatever|does) \\cdot  p(a|whatever) \\cdot  p(spider|a) \\cdot p(can|spider)$$\n",
    "\n",
    "And for the trigram model:\n",
    "$$p(spiderman,spiderman)\\cdot p(does|spiderman,spiderman) \\cdot  p(whatever|spiderman,does) \\cdot p(a|does,whatever) \\cdot  p(spider|whatever,a) \\cdot  p(can|a, spider)$$\n",
    "\n",
    "Since we do not want a zero probability sentence use Laplace smoothing, as you have seen in the lecture, or here https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram log likelihood is -35.159414861051204\n",
      "Trigram log likelihood is -32.68473273073879\n"
     ]
    }
   ],
   "source": [
    "# for the probability smoothing\n",
    "NUMERATOR_SMOOTHING = 1 # alpha in https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "DENOMINATOR_SMOOTHING = 10**4 # d in https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "def sentence_log_probabilty(unigrams : Counter, bigrams  : Counter,trigrams : Counter, sentence: str):\n",
    "    bigram_log_likelilhood, trigram_log_likelilhood = 0, 0\n",
    "    words_in_sentence = sentence.split()\n",
    "    n_words = len(words_in_sentence)\n",
    "    for i, word in  enumerate(words_in_sentence):\n",
    "        ### YOUR CODE HERE\n",
    "        if i==0:\n",
    "            words_counter = np.sum(np.array(list(unigrams.values())))\n",
    "            #bigram_log_likelilhood=1\n",
    "            #bigram_log_likelilhood *= ((unigrams[word]+NUMERATOR_SMOOTHING)/(words_counter+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING)))\n",
    "            bigram_log_likelilhood += np.log10(((unigrams[word]+NUMERATOR_SMOOTHING)/(words_counter+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))))\n",
    "        if i==1:\n",
    "            words_counter = np.sum(np.array(list(unigrams.values())))\n",
    "            #bigram_log_likelilhood *= (bigrams[(words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/((unigrams[words_in_sentence[i-1]])+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "            bigram_log_likelilhood += np.log10((bigrams[(words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/((unigrams[words_in_sentence[i-1]])+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING)))\n",
    "            #trigram_log_likelilhood=1\n",
    "            #trigram_log_likelilhood *= (bigrams[(words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/(words_counter+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "            trigram_log_likelilhood += np.log10((bigrams[(words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/(words_counter+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING)))\n",
    "\n",
    "        if i>=2:\n",
    "            #bigram_log_likelilhood *= (bigrams[(words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/((unigrams[words_in_sentence[i-1]])+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "            bigram_log_likelilhood += np.log10((bigrams[(words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/((unigrams[words_in_sentence[i-1]])+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING)))\n",
    "            #trigram_log_likelilhood *= (trigrams[(words_in_sentence[i-2],words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/((bigrams[(words_in_sentence[i-1],word)])+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "            trigram_log_likelilhood += np.log10((trigrams[(words_in_sentence[i-2],words_in_sentence[i-1],word)]+NUMERATOR_SMOOTHING)/((bigrams[(words_in_sentence[i-1],word)])+(NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING)))\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "    return (bigram_log_likelilhood,trigram_log_likelilhood)\n",
    "    \n",
    "sentence = \"spider man spider man does whatever a spider can\"\n",
    "bigram_log_likelilhood, trigram_log_likelilhood = sentence_log_probabilty(tf_idf.unigram_count, tf_idf.bigram_count, tf_idf.trigram_count, sentence)\n",
    "print(F\"Bigram log likelihood is {bigram_log_likelilhood}\")\n",
    "print(F\"Trigram log likelihood is {trigram_log_likelilhood}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.51 Language model: B\n",
    "For each model what is the next word prediciton for the sentnence \"like big\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the bigram model the next word prediction is: big\n",
      "For the trigram model the next word prediction is: deal\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "words_counter = tf_idf.unigram_count.keys()\n",
    "max_big_bigram=0\n",
    "max_likebig_trigram=0\n",
    "for i, word in enumerate(words_counter):\n",
    "        if tf_idf.bigram_count[(\"big\",word)]>max_big_bigram:\n",
    "            max_big_bigram=tf_idf.bigram_count[(\"big\",word)]\n",
    "            pred_word_bigram=word\n",
    "        if tf_idf.trigram_count[(\"like\",\"big\",word)]>max_likebig_trigram:\n",
    "            max_likebig_trigram=tf_idf.trigram_count[(\"like\",\"big\",word)]\n",
    "            pred_word_trigram=word\n",
    "                           \n",
    "print(\"For the bigram model the next word prediction is: %s\" %pred_word_bigram)\n",
    "print(\"For the trigram model the next word prediction is: %s\" %pred_word_trigram)\n",
    "        \n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
